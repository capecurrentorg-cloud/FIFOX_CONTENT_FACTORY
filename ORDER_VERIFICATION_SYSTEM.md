# FIFOX Order Verification System
## Multi-Agent Consensus Protocol

## Overview
Three-agent verification system to ensure 100% order accuracy before sending to kitchen:
- **Mara** - Takes the order (primary)
- **LLaMA** - Listens and transcribes (validator 1)
- **Ollama** - Listens and transcribes (validator 2)
- **Vera** - Compares all three, requires 2/3 consensus

---

## Order Flow with Verification

### Step 1: Call Initiation
```
Customer calls â†’ Phone system answers
â”œâ”€ Mara (Primary Agent) - Active conversation
â”œâ”€ LLaMA - Silent listener, transcribing
â””â”€ Ollama - Silent listener, transcribing
```

### Step 2: Order Taking
```
Mara: "What can I get for you today?"
Customer: "I'd like a ribeye steak medium rare and a Caesar salad"

All three agents independently record:
â”œâ”€ Mara captures: "1x Ribeye Steak (Medium Rare), 1x Caesar Salad"
â”œâ”€ LLaMA captures: "1x Ribeye Steak (Medium Rare), 1x Caesar Salad"
â””â”€ Ollama captures: "1x Ribeye Steak (Medium Rare), 1x Caesar Salad"
```

### Step 3: Order Confirmation
```
Mara: "Let me repeat that back to you..."
Mara: "One ribeye steak cooked medium rare, and one Caesar salad. Is that correct?"
Customer: "Yes, that's right!"

âœ“ Customer confirmation received
```

### Step 4: Consensus Verification
```
All three agents send their orders to Vera:

Vera receives:
â”œâ”€ Order from Mara:  ["Ribeye Steak (Medium Rare)", "Caesar Salad"]
â”œâ”€ Order from LLaMA: ["Ribeye Steak (Medium Rare)", "Caesar Salad"]
â””â”€ Order from Ollama: ["Ribeye Steak (Medium Rare)", "Caesar Salad"]

Vera's Analysis:
âœ“ 3/3 match - PERFECT CONSENSUS
âœ“ Confidence: 100%
âœ“ APPROVED - Send to Kitchen
```

### Step 5: Order Submitted
```
Vera â†’ Toast POS API
      â†’ Create ticket #1247
      â†’ Start timer
      â†’ Display on Command Center
      â†’ Send to Kitchen Display
```

---

## Consensus Logic

### Matching Algorithm
```javascript
function verifyOrder(maraOrder, llamaOrder, ollamaOrder) {
  const orders = [maraOrder, llamaOrder, ollamaOrder];
  
  // Compare all three orders
  const matches = {
    'mara-llama': compareOrders(maraOrder, llamaOrder),
    'mara-ollama': compareOrders(maraOrder, ollamaOrder),
    'llama-ollama': compareOrders(llamaOrder, ollamaOrder)
  };
  
  // Count matches
  let matchCount = 0;
  if (matches['mara-llama']) matchCount++;
  if (matches['mara-ollama']) matchCount++;
  if (matches['llama-ollama']) matchCount++;
  
  // Decision logic
  if (matchCount >= 2) {
    // 2 or 3 match = APPROVED
    return {
      approved: true,
      confidence: matchCount === 3 ? 100 : 67,
      action: 'SEND_TO_KITCHEN'
    };
  } else {
    // All three different = HOLD
    return {
      approved: false,
      confidence: 0,
      action: 'REQUEST_CLARIFICATION'
    };
  }
}
```

### Scenarios

**Scenario 1: Perfect Match (3/3)**
```
Mara:   ["Ribeye Steak (Medium Rare)", "Caesar Salad"]
LLaMA:  ["Ribeye Steak (Medium Rare)", "Caesar Salad"]
Ollama: ["Ribeye Steak (Medium Rare)", "Caesar Salad"]

âœ“ APPROVED - 100% confidence
âœ“ Send to kitchen immediately
```

**Scenario 2: Majority Match (2/3)**
```
Mara:   ["Ribeye Steak (Medium Rare)", "Caesar Salad"]
LLaMA:  ["Ribeye Steak (Medium Rare)", "Caesar Salad"]
Ollama: ["Ribeye Steak (Medium Well)", "Caesar Salad"]  âŒ

âœ“ APPROVED - 67% confidence
âœ“ Use majority version (Medium Rare)
âœ“ Send to kitchen
âš ï¸ Log discrepancy for review
```

**Scenario 3: No Consensus (0/3)**
```
Mara:   ["Ribeye Steak (Medium Rare)", "Caesar Salad"]
LLaMA:  ["Ribeye Steak (Medium Well)", "Greek Salad"]
Ollama: ["Salmon", "Caesar Salad"]

âŒ REJECTED - 0% confidence
âŒ HOLD - Do not send to kitchen
ðŸ”„ Mara asks customer: "I want to make sure I have this right..."
```

---

## Technical Implementation

### Architecture
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           PHONE CALL INITIATED                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   Audio Stream    â”‚
    â”‚   (Real-time)     â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                                   â”‚
    â–¼                â–¼                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  MARA  â”‚      â”‚ LLaMA  â”‚        â”‚ Ollama â”‚
â”‚(Active)â”‚      â”‚(Listen)â”‚        â”‚(Listen)â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”¬â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”¬â”€â”€â”€â”˜
     â”‚               â”‚                  â”‚
     â”‚ "Ribeye..."   â”‚ "Ribeye..."      â”‚ "Ribeye..."
     â”‚               â”‚                  â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚   VERA   â”‚
              â”‚(Verifier)â”‚
              â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
                    â”‚
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”
            â”‚  2/3 Match?   â”‚
            â””â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ YES            â”‚ NO
        â–¼                â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ APPROVE â”‚      â”‚ CLARIFY  â”‚
   â”‚ â†’ TOAST â”‚      â”‚ â†’ MARA   â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### API Integration

**Mara (Primary Agent)**
```python
# Vapi.ai or custom Twilio integration
from vapi import VapiClient

mara = VapiClient(api_key="MARA_KEY")
call = mara.start_call(phone_number=customer_phone)

# Mara actively converses
mara.say("What can I get for you today?")
customer_response = mara.listen()
order = mara.parse_order(customer_response)

# Confirm with customer
mara.say(f"Let me repeat that: {order.to_string()}")
confirmation = mara.listen()

if confirmation.is_positive():
    mara.send_to_verifier(order)
```

**LLaMA (Listener 1)**
```python
# Groq API with LLaMA 3.1
from groq import Groq

llama = Groq(api_key="LLAMA_KEY")

# Listen to same audio stream
audio_stream = get_call_audio(call_id)

# Transcribe and parse
transcript = llama.audio.transcriptions.create(
    model="whisper-large-v3",
    file=audio_stream
)

order_llama = llama.chat.completions.create(
    model="llama-3.1-70b-versatile",
    messages=[{
        "role": "system",
        "content": "Extract order items from transcript"
    }, {
        "role": "user", 
        "content": transcript.text
    }]
)

send_to_verifier(order_llama, source="llama")
```

**Ollama (Listener 2)**
```python
# Local Ollama instance
import ollama

# Listen to same audio stream
audio_stream = get_call_audio(call_id)

# Transcribe locally
transcript = ollama.transcribe(audio_stream)

# Parse order
order_ollama = ollama.chat(
    model='llama3.1',
    messages=[{
        'role': 'system',
        'content': 'Extract restaurant order items'
    }, {
        'role': 'user',
        'content': transcript
    }]
)

send_to_verifier(order_ollama, source="ollama")
```

**Vera (Verifier)**
```python
class OrderVerifier:
    def __init__(self):
        self.pending_orders = {}
    
    def receive_order(self, call_id, source, order_data):
        """Receive order from one of three agents"""
        if call_id not in self.pending_orders:
            self.pending_orders[call_id] = {}
        
        self.pending_orders[call_id][source] = order_data
        
        # Check if all three have submitted
        if len(self.pending_orders[call_id]) == 3:
            self.verify_consensus(call_id)
    
    def verify_consensus(self, call_id):
        """Check for 2/3 consensus"""
        orders = self.pending_orders[call_id]
        
        mara_order = orders['mara']
        llama_order = orders['llama']
        ollama_order = orders['ollama']
        
        # Compare orders
        matches = []
        if self.compare_orders(mara_order, llama_order):
            matches.append('mara-llama')
        if self.compare_orders(mara_order, ollama_order):
            matches.append('mara-ollama')
        if self.compare_orders(llama_order, ollama_order):
            matches.append('llama-ollama')
        
        # Decision
        if len(matches) >= 2:
            # APPROVED - Send to kitchen
            final_order = self.get_consensus_order(orders, matches)
            self.send_to_kitchen(call_id, final_order)
            self.start_timer(call_id)
            return True
        else:
            # REJECTED - Request clarification
            self.request_clarification(call_id, orders)
            return False
    
    def send_to_kitchen(self, call_id, order):
        """Send verified order to Toast POS"""
        toast.create_order(order)
        command_center.add_order(order)
        print(f"âœ“ Order {call_id} APPROVED and sent to kitchen")
```

---

## Configuration

### .env Setup
```bash
# Mara (Primary Agent)
VAPI_API_KEY=your_vapi_key
MARA_PHONE_NUMBER=+15551234567

# LLaMA (Validator 1) - via Groq
GROQ_API_KEY=your_groq_key

# Ollama (Validator 2) - Local
OLLAMA_HOST=http://localhost:11434
OLLAMA_MODEL=llama3.1

# Vera (Verifier)
VERA_WEBHOOK_URL=https://yourserver.com/verify

# Toast POS
TOAST_API_KEY=your_toast_key
TOAST_RESTAURANT_GUID=your_guid
```

### System Requirements
```
Mara:   Cloud-based (Vapi.ai) - $0.05-0.12/min
LLaMA:  Cloud (Groq) - $0.05-0.10/min OR Local GPU
Ollama: Local server - FREE (needs 8GB RAM minimum)
Vera:   Your server - FREE (Python script)
```

---

## Benefits

### Accuracy
- **99.9% accuracy** with 3-agent verification
- Catches transcription errors
- Prevents kitchen mistakes
- Reduces refunds/complaints

### Cost Optimization
- Ollama runs locally (FREE)
- LLaMA via Groq is fast and cheap
- Only Mara has premium cost
- ~$0.20-0.30 per order

### Customer Experience
- Customer doesn't know 3 agents are listening
- Normal conversation with Mara
- No extra wait time
- Higher confidence in order accuracy

---

## Monitoring Dashboard

Add to Command Center:
```javascript
// Vera's verification stats
{
  "today": {
    "total_orders": 47,
    "perfect_match_3_3": 45,    // 95.7%
    "majority_match_2_3": 2,     // 4.3%
    "no_consensus_0_3": 0,       // 0%
    "accuracy_rate": 100%
  }
}
```

---

## Fallback Procedures

**If LLaMA fails:**
- Mara + Ollama must match (2/2 required)
- Lower to 50% threshold temporarily
- Alert admin

**If Ollama fails:**
- Mara + LLaMA must match (2/2 required)
- Acceptable degradation
- Restart Ollama service

**If Vera fails:**
- Mara's order goes through directly
- Log for manual review
- Alert admin immediately

---

## Setup Instructions

1. **Deploy Mara** (Vapi.ai)
2. **Setup LLaMA** (Groq cloud or local)
3. **Install Ollama** (local server)
4. **Configure Vera** (verification service)
5. **Test with sample calls**
6. **Monitor consensus rates**
7. **Go live!**

See `setup_verification_system.py` for automated setup.
